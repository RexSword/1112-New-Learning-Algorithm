{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5duVCymD0blZ81y9QyDsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RexSword/1112-New-Learning-Algorithm/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW3\n",
        "## weight-tuning_LG\n",
        "\n",
        "Model 6 \\\n",
        "hidden nodes: 11 \\\n",
        "epochs: 300 \\\n",
        "init: xavier \\\n",
        "active: relu \\\n",
        "optimize: sgd \\\n",
        "schedule: None \\\n",
        "weight decay: 0.0"
      ],
      "metadata": {
        "id": "9fqU9FCbwoJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the functions"
      ],
      "metadata": {
        "id": "2-h7sD-Izpow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcDi422YvORv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "Y2bZB0XXz5h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method: Callable, active_func: nn.modules.module.Module) -> None:\n",
        "        super(TwoLayerNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # full connected first layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # activation\n",
        "        self.active_func = active_func()\n",
        "        # initialize\n",
        "        for param in self.parameters():\n",
        "            init_method(param)\n",
        "        # full connected second layer\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.active_func(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "3GJQs1lQwFrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "QA_9OLFcz733"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from array import array\n",
        "\n",
        "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay: float, learning_goal: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        learning goal: the desire ratio of [validation loss / initiate validation loss] to early stop\n",
        "    \"\"\"\n",
        "    if epochs < 1:\n",
        "        raise ValueError(\"Invalid epoch!!\")\n",
        "    else:\n",
        "        epochs = int(epochs)\n",
        "    model.to(device)\n",
        "    optimizer = opt(model.parameters(), lr=learning_rate,\n",
        "                    weight_decay=weight_decay)\n",
        "    scheduler = sched(optimizer) if sched else None\n",
        "    history = tuple(array(\"d\", [0] * epochs) for e in range(4))\n",
        "    # Train the model\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        model.train()\n",
        "        for X, y in trainloader:\n",
        "            X = X.view(-1, model.input_size).to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_correct += (predicted == y).sum().item()\n",
        "        train_loss /= len(trainloader.dataset)\n",
        "        train_accuracy = train_correct / len(trainloader.dataset)\n",
        "\n",
        "        # Validate the model\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X, y in valloader:\n",
        "                X = X.view(-1, model.input_size).to(device)\n",
        "                y = y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == y).sum().item()\n",
        "            val_loss /= len(valloader.dataset)\n",
        "            val_accuracy = val_correct / len(valloader.dataset)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        # Print epoch statistics\n",
        "        history[0][epoch] = train_loss\n",
        "        history[1][epoch] = train_accuracy\n",
        "        history[2][epoch] = val_loss\n",
        "        history[3][epoch] = val_accuracy\n",
        "        if learning_goal * history[2][0] > val_loss:\n",
        "            return history\n",
        "        # sys.stdout.write('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\\n'\n",
        "        #       .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n",
        "    return history"
      ],
      "metadata": {
        "id": "QEJlTw1JzI5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training_UA"
      ],
      "metadata": {
        "id": "IFqw520F27wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from array import array\n",
        "\n",
        "def train_UA(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay: float, learning_goal: float):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        learning goal: the desire ratio of [validation loss / initiate validation loss] to early stop\n",
        "    \"\"\"\n",
        "    if epochs < 1:\n",
        "        raise ValueError(\"Invalid epoch!!\")\n",
        "    else:\n",
        "        epochs = int(epochs)\n",
        "    model.to(device)\n",
        "    optimizer = opt(model.parameters(), lr=learning_rate,\n",
        "                    weight_decay=weight_decay)\n",
        "    scheduler = sched(optimizer) if sched else None\n",
        "    history = tuple(array(\"d\", [0] * epochs) for e in range(4))\n",
        "\n",
        "    prev_loss = None\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        model.train()\n",
        "        for X, y in trainloader:\n",
        "            X = X.view(-1, model.input_size).to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_correct += (predicted == y).sum().item()\n",
        "        train_loss /= len(trainloader.dataset)\n",
        "        train_accuracy = train_correct / len(trainloader.dataset)\n",
        "\n",
        "        # Validate the model\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X, y in valloader:\n",
        "                X = X.view(-1, model.input_size).to(device)\n",
        "                y = y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == y).sum().item()\n",
        "            val_loss /= len(valloader.dataset)\n",
        "            val_accuracy = val_correct / len(valloader.dataset)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Adjust the learning rate based on the validation loss\n",
        "        if  val_loss is None && val_loss < prev_loss:\n",
        "            learning_rate *= 1.2\n",
        "            prev_loss = val_loss\n",
        "            print(f\"Adjusting learning rate to {learning_rate:.6f}\")\n",
        "        else:\n",
        "            learning_rate *= 0.7\n",
        "            print(f\"Adjusting learning rate to {learning_rate:.6f}\")\n",
        "         \n",
        "        \n",
        "        # Print epoch statistics\n",
        "        history[0][epoch] = train_loss\n",
        "        history[1][epoch] = train_accuracy\n",
        "        history[2][epoch] = val_loss\n",
        "        history[3][epoch] = val_accuracy\n",
        "        if learning_goal * history[2][0] > val_loss:\n",
        "            return history\n",
        "        # sys.stdout.write('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\\n'\n",
        "        #       .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n",
        "    return history"
      ],
      "metadata": {
        "id": "7GGNlScA27Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "bvaR1XmDz-5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model: nn.Module, device: str, testloader: DataLoader):\n",
        "    val_correct = 0\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in testloader:\n",
        "            X = X.view(-1, model.input_size).to(device)\n",
        "            y = y.to(device)\n",
        "            outputs = model(X)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == y).sum().item()\n",
        "        val_accuracy = val_correct / len(testloader.dataset)\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "moswz8dCzOFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "79P_YxDe0DCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pytorch dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def getPytorchData(train: float = 0.8, remain: float = 0.1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train: train_amount / total_amount or 1 - valid_amount / total_amount\n",
        "        remain: reduce data amount to save time\n",
        "    \"\"\"\n",
        "    # preprocess: flatten, normalize, drop 90%, split\n",
        "    transform = transforms.transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    if 0 >= train or train >= 1:\n",
        "        raise ValueError()\n",
        "    if 0 > remain or remain > 1:\n",
        "        raise ValueError()\n",
        "    # Split the training set into training and validation sets\n",
        "    trainset = datasets.FashionMNIST(\n",
        "        root=\"./data/\", train=True, download=False, transform=transform)\n",
        "    train_count = int(train * remain * len(trainset))\n",
        "    valid_count = int((1 - train) * remain * len(trainset))\n",
        "    if train_count * valid_count == 0:\n",
        "        raise ValueError()\n",
        "    datum_size = product(trainset[0][0].size())\n",
        "    class_amount = len(trainset.classes)\n",
        "    testset = datasets.FashionMNIST(\n",
        "        root=\"./data/\", train=False, download=False, transform=transform)\n",
        "    print(train_count, valid_count, len(testset))\n",
        "    trainset, valset, _ = random_split(\n",
        "        trainset, (train_count, valid_count, len(trainset) - train_count - valid_count), Generator().manual_seed(42))\n",
        "    # Create dataloaders to load the data in batches\n",
        "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
        "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
        "    return trainloader, valloader, testloader, datum_size, class_amount"
      ],
      "metadata": {
        "id": "y5PHYLGD0J_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training process"
      ],
      "metadata": {
        "id": "plPEunoO0YzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available(\n",
        ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# model spec\n",
        "trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
        "hidden_size = 11\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set the hyperparameter\n",
        "init = lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None\n",
        "active = nn.ReLU\n",
        "optimize = optim.SGD\n",
        "schedule = None\n",
        "weight_decay = 0.0\n"
      ],
      "metadata": {
        "id": "V3TQE7o80Xvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### weigth-tuning_LG"
      ],
      "metadata": {
        "id": "46cCijT31mr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "learning_goal = 0.3\n",
        "\n",
        "model = TwoLayerNetwork(input_size, hidden_size,\n",
        "                        output_size, init, active)\n",
        "LG_baseline = test(model, device, testloader)\n",
        "LG_history = train(model, optimize, device, epochs, learning_rate,\n",
        "                   trainloader, valloader, criterion, schedule, weight_decay, learning_goal)\n",
        "LG_result = test(model, device, testloader)\n",
        "print(LG_baseline, LG_history, LG_result, sep=\"\\n\")\n"
      ],
      "metadata": {
        "id": "aaghcnci0rwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### weight-tuning_LG_UA"
      ],
      "metadata": {
        "id": "fYBRBnn42xsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5uNxWiD23Sp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}